http {
  host = "0.0.0.0"
  host = ${?HTTP_HOST}

  port = 8080
  port = ${?HTTP_PORT}
}

producer {
  topics = test
  topics = ${?PRODUCER_TOPICS}

  metadata.broker.list=embedded

  # Compression code being used
  # 0: NoCompressionCodec
  # 1: GZIPCompressionCodec
  # 2: SnappyCompressionCodec
  compression.codec=0

  # sync  = message send requests to the brokers are synchronously, one at a time as they arrive.
  # async = message send requests to the brokers are asynchronously
  producer.type=async

  # request required acks
  request.required.acks=0

  serializer.class=kafka.serializer.StringEncoder

  # The ack timeout of the producer requests. Value must be non-negative and non-zero
  request.timeout.ms=1500

  # The size of the tcp RECV size.
  send.buffer.bytes=102400
}

consumer {
  topics = test
  topics = ${?CONSUMER_TOPICS}

  # Comma separated of zookeeper nodes
  # provide an empty list for using the EmbeddedKafka
  zookeeper.connect=embedded
  zookeeper.session.timeout.ms=400
  zookeeper.sync.time.ms=200

  # The group id of this consumer
  group.id=kafka-http-0

  # The size of the tcp RECV size.
  socket.receive.buffer.bytes=2097152

  # The socket timeout used for the connection to the broker
  socket.timeout.ms=30000

  # If set offsets will be commited automatically during consuming
  auto.commit.enable=true

  # The time interval at which to save the current offset in ms
  auto.commit.interval.ms=10000

  # smallest: If the consumer does not already have an established offset to consume from
  #           start with the earliest message present in the log rather than the latest message.
  #
  # largest: start with the latest message from the log
  auto.offset.reset=smallest

  # The min number of bytes each fetch request waits for.
  fetch.min.bytes=1

  # The max amount of time each fetch request waits.
  fetch.wait.max.ms=100

  # consumer throws timeout exception after waiting this much of time without incoming messages
  consumer.timeout.ms=1000

}